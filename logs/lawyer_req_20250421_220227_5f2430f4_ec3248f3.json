// ============================================================
// LAWYER MATCHING REQUEST METADATA
// ============================================================
// Timestamp: 2025-04-21T22:02:27.070391
// Request ID: ec3248f3
// User ID: 5f2430f4
// Client IP: 127.0.0.1
// Client Hostname: DESKTOP-9BPBDO7
// 
REQUEST HEADERS:
//   Host: 127.0.0.1:8000
//   Connection: close
//   Content-Length: 36321
//   Sec-Ch-Ua-Platform: "Windows"
//   Authorization: Bearer eyJhbGciOiJIUzI1NiIsImtpZCI6IlAyRnNtdXhwRmI2YXBMUWgiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2tlaHJsaGV5bnhwYnRxenhkanV5LnN1cGFiYXNlLmNvL2F1dGgvdjEiLCJzdWIiOiI1ZjI0MzBmNC00NGQ2LTQ4NTItYmEwNy1mNTUwYmYyNmRiMDEiLCJhdWQiOiJhdXRoZW50aWNhdGVkIiwiZXhwIjoxNzQ1MjkzODIwLCJpYXQiOjE3NDUyOTAyMjAsImVtYWlsIjoicnlhbmJhcnJldHRvNzBAZ21haWwuY29tIiwicGhvbmUiOiIiLCJhcHBfbWV0YWRhdGEiOnsicHJvdmlkZXIiOiJlbWFpbCIsInByb3ZpZGVycyI6WyJlbWFpbCIsImdvb2dsZSJdfSwidXNlcl9tZXRhZGF0YSI6eyJhdmF0YXJfdXJsIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EvQUNnOG9jTFJUdDV2c1hwVmI4WkpEcEZRcnFXZWdyeFhCVS14VTNWYlFRcEI5REtyZ0dyUGh3PXM5Ni1jIiwiZW1haWwiOiJyeWFuYmFycmV0dG83MEBnbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiZnVsbF9uYW1lIjoiUnlhbiBCYXJyZXR0byIsImlzcyI6Imh0dHBzOi8vYWNjb3VudHMuZ29vZ2xlLmNvbSIsIm5hbWUiOiJSeWFuIEJhcnJldHRvIiwicGhvbmVfdmVyaWZpZWQiOmZhbHNlLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EvQUNnOG9jTFJUdDV2c1hwVmI4WkpEcEZRcnFXZWdyeFhCVS14VTNWYlFRcEI5REtyZ0dyUGh3PXM5Ni1jIiwicHJvdmlkZXJfaWQiOiIxMDc2MjMxMzcyNzAwMzM4NDI2MDEiLCJzdWIiOiIxMDc2MjMxMzcyNzAwMzM4NDI2MDEifSwicm9sZSI6ImF1dGhlbnRpY2F0ZWQiLCJhYWwiOiJhYWwxIiwiYW1yIjpbeyJtZXRob2QiOiJvYXV0aCIsInRpbWVzdGFtcCI6MTc0NTI4MTkyOH1dLCJzZXNzaW9uX2lkIjoiZDI0NWI4MDQtMTBmZS00YTU4LWJkYjYtNDFjYzVkMmFkY2FhIiwiaXNfYW5vbnltb3VzIjpmYWxzZX0.V7nIlq_RDEH5NBshMCOZ2lhEOHtIw3T6UXIq8CPPPy0
//   User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36
//   Sec-Ch-Ua: "Google Chrome";v="135", "Not-A.Brand";v="8", "Chromium";v="135"
//   Content-Type: application/json
//   Sec-Ch-Ua-Mobile: ?0
//   Accept: */*
//   Origin: http://localhost:3000
//   Sec-Fetch-Site: same-origin
//   Sec-Fetch-Mode: cors
//   Sec-Fetch-Dest: empty
//   Referer: http://localhost:3000/lawyer-search
//   Accept-Encoding: gzip, deflate, br, zstd
//   Accept-Language: en-US,en;q=0.9
//   X-Real-Ip: ::1
//   X-Vercel-Deployment-Url: localhost:3000
//   X-Vercel-Forwarded-For: ::1
//   X-Vercel-Id: dev1::ihmko-1745290946762-e7f03df613ad
//   X-Forwarded-For: ::1
//   X-Forwarded-Port: 3000
//   X-Forwarded-Proto: http
//   X-Forwarded-Host: [::1]:49840
// ============================================================


{
  "user_id": "5f2430f4-44d6-4852-ba07-f550bf26db01",
  "uploaded_documents": {
    "resume": true,
    "publications": true,
    "awards": false,
    "recommendation": false,
    "press": false,
    "salary": false,
    "judging": false,
    "membership": false,
    "contributions": false
  },
  "document_summaries": {
    "publications": {
      "extracted_text": "04 OCT 2024  |  VOL 7  |  1\nJournal of Emerging Investigators  \u2022  www.emerginginvestigators.org\nArticleLLM that was built on BERT, is currently ranked seventh for \nthe General Language Understanding Evaluation (GLUE) \nbenchmark (3,4). The GLUE benchmark is a large dataset \nthat creates several fundamental tasks of natural language \nprocessing (NLP) including question answering and text \nclassification, amongst others. The base version of the BERT \nmodel comprises 86 million total parameters while the large \nversion has over 300 million parameters. To be able to fine-tune \nsuch a model on a task, a GPU and a considerable amount of \ntime is required. Additionally, achieving optimal performance \nrequires hyper parameter tweaking, such as training for more/\nless epochs (training iterations) and increasing/decreasing \nthe learning rate (how aggressive the training approach is), \nwhich further the total time complexity. Traditional methods \nattempt to use classical machine learning algorithms (as \nopposed to deep learning neural networks) paired with text \nfeature extraction tools, such as term frequency \u2013  inverse \ndocument frequency vectorizers (TF\u2013IDF) to model textual \ndata (5). These methods are computationally simpler than \ncurrent state of the art techniques, but do not achieve the \nsame level of performance. The primary issue with such \nmethods is that they fail to incorporate textual relationships \ninto the features they provide. However, these methods do \nindicate that machine learning methods can be viable so \nlong as meaningful features are provided. Methods that \nutilize information from the writing process, rather than just \ninformation from the produced text itself, do exist and have \nachieved success. Keystroke logging is the act of recording \nkeyboard events while a user is typing. The data collected \nfrom keystroke loggers can be very informative and reveal \ninformation about textual data that the text itself cannot (6). \nFor example, suppose a writer takes a deep pause when \nwriting, perhaps planning out the rest of their essay or for \nsome other deliberation. Looking at just the written text, it \nwould be impossible to assess that such a pause was ever \ntaken. However, with keystroke log data, one can analyze the \ntime between clicked keys and discover such events.\n Due to the differences in the way current LLM methods \nand keystroke log methods approach text-based problems, \na hybrid approach could improve the results of both. LLM \nmethods offer a detailed look into the text itself and the \ndeeply encoded properties within it, while the keystroke log \napproach uncovers details about the writing process, missing \nfrom the final product. A combination of both should therefore \nprovide the best results on downstream tasks. However, if \nresources are limited, an approach solely using keystroke log \ndata could be advisable as one can fully train a model and \nperform inference with just a CPU in a very short amount of \ntime. Therefore, the use of keystroke log data could not only \nserve as a source of improvement on NLP performance but \ncould also serve as a speedy alternative in many situations.Gradient boosting with temporal feature extraction for \nmodeling keystroke log data\nSUMMARY\nAlthough there has been great progress in the field of \nNatural language processing (NLP) over the last few \nyears, particularly with the development of attention-\nbased models, less research has contributed \ntowards modeling keystroke log data. State of the art \nmethods handle textual data directly and while this \nhas produced excellent results, the time complexity \nand resource usage are quite high for such methods. \nAdditionally, these methods fail to incorporate the \nactual writing process when assessing text and instead \nsolely focus on the content. Therefore, we proposed a \nframework for modeling textual data using keystroke-\nbased features. Such methods pay attention to how \na document or response was written, rather than the \nfinal text that was produced. These features are vastly \ndifferent from the kind of features extracted from raw \ntext but reveal information that is otherwise hidden. \nWe hypothesized that pairing efficient machine \nlearning techniques with keystroke log information \nshould produce results comparable to transformer \ntechniques, models which pay more or less attention \nto the different components of a text sequence in \na far quicker time. Transformer-based methods \ndominate the field of NLP currently due to the strong \nunderstanding they display of natural language. We \nshowed that models trained on keystroke log data are \ncapable of effectively evaluating the quality of writing \nand do it in a significantly shorter amount of time \ncompared to traditional methods. This is significant \nas it provides a necessary fast and cheap alternative \nto increasingly larger and slower LLMs.\nINTRODUCTION\n Transformer attention-based methods have largely \ndominated text-based supervised learning over the last \nfew years, mainly due to publicly available large language \nmodels (LLMs) (1, 2). These models allow for highly efficient \nand effective fine tuning on downstream tasks, such as \nsequence classification and question answering. Despite the \nreduced time it takes to fine tune LLMs relative to training \na model entirely from scratch, the training process can still \ntake considerable time and requires support from a graphics \nprocess unit (GPU).\n Bidirectional encoder representations from transformers \n(BERT) were one of the first LLMs and also one of the first \nmodels to utilize attention, which gave it the unique ability of \nunderstanding how words relate to one another. Decoding-\nenhanced BERT with disentangled attention (DeBERTa), an Ryan Barretto1, Sanjay Barretto1\n1 William Fremd High School, Inverness, Illinois\n04 OCT 2024  |  VOL 7  |  2\nJournal of Emerging Investigators  \u2022  www.emerginginvestigators.org\nhttps://doi.org/10.59720/24-087  Prior research has been conducted on utilizing keystroke \nlog data for modeling certain downstream tasks. Positive \nrelationships have been found between keystroke information \nand students\u2019 writing processes by modeling keystroke \nlog data with heavy tailed probability distributions (7). \nSpecifically, work has demonstrated that task engagement \nand writing efforts may play a big role in the overall quality of \nwriting produced (8). Other research focused on assessing \nstudent writing quality by modeling features extracted from \nkeystroke log data (9). This work demonstrated promising \nresults for uncovering relationships between keystroke log \ndata and writing quality. Each of these works suggest that \nkeystroke information may have strong predictive power for \nthe quality of writing, However, due to the older models that \nwere used in these papers, we believe that the results could \nbe improved upon with newer, advanced machine learning \nmodels. Research has also shown that just the number of \nwords written in an essay within the first 999 keystrokes could \nbe powerful predictors for machine learning models (10). On a \nsimilar note, work has shown writing quality could be modeled \nsuccessfully with just two to five features (variables that can \nbe manipulated to make predictions), with the number of \nkeystrokes for a given essay being the most important (11). \nResearch in 2022 looked at identifying students in need \nof assistance using a variety of features similar to those \nextracted from the 2022 study, using models such as Naive \nBayes, SVM\u2019s, and random forests (12). While this research \nwas insightful, the methods did not address assessing the \nquality of writing.\n Recently, research has been conducted examining the use \nof keystroke dynamics paired with machine learning models \n(13). In this research, the authors use classical machine \nlearning models like SVM\u2019s, random forests, and K-nearest \nneighbors, in addition to deep learning methods like neural \nnetworks. These models were used to analyze keystroke log \ndata to distinguish between the different users who typed \nresponses. The authors ultimately offered a comparative \nanalysis of different models for biometric identification \n(determining the identity of a person), while we have focused \non a single gradient booster\u2019s performance on keystroke log \ndata for analyzing the quality of students\u2019 writing versus the \nperformance of LLMs on the same task. \n We hypothesized that pairing efficient machine learning \ntechniques with keystroke log information will produce \nresults comparable to transformer techniques in less \ntime. We demonstrated that gradient boosters, machine \nlearning algorithms that merge the ideas of neural networks \nand random forests into one algorithm, using keystroke \ninformation are able to produce strong results on assessing \nstudent writing in under a second. It is clear that keystroke log \ndata, normally consisting of keypress speed and frequency, \nwhich describes the writing process, can be just as useful \nfor understanding student writing as the actual writing itself. \nOur results suggest the writing process reveals important \ninformation that is often missed in traditional NLP techniques, \nspecifically within the task of evaluating the quality of writing. \nThis additional information could be helpful not only by itself \nfor efficient and fast data modeling as shown in this paper, but \nalso paired with the original text may provide the entire picture \nof a student\u2019s writing which could lead to improvements on \nmodeling in the field as a whole.RESULTS\n Raw keystroke data consists of the type of key event \ntaking place over time and is then grouped according to the \nsession it took place. Features that were extracted from this \nraw data involve the number of keystroke events, average \ntime taken per keystroke and more ( Table 1 ). A gradient \nboosting classifier was then trained on this extracted data \nto distinguish between different qualities of writing. This \nparticular classifier was chosen because it has been shown to \nbe extremely efficient and accurate when dealing with tabular \ndata. The main metrics for evaluating the model are the root \nmean square error (RMSE) and quadratic weighted kappa \n(QWK). \n The major results looked at are the times it took for a \ncertain model to train and the scores it achieved in predicting \nthe quality of an essay given the actual quality it received by \nhuman raters. On average, each model scored an RMSE \nof 0.6624 taking a total of 0.3774 seconds to train (Figure \n1). The normalized RMSE for each model, on average, was \n0.1104, due to the range of our target variable being equal \nTable 1: Base-level extracted features synthesized from general \npatterns discovered within the raw keystroke log data. These \nfeatures represent simple patterns that were recovered from an \nindividual's writing process. Most features relate to the time it took \nfor keystroke events as well as the types of keystrokes occurring.\n04 OCT 2024  |  VOL 7  |  3\nJournal of Emerging Investigators  \u2022  www.emerginginvestigators.org\nhttps://doi.org/10.59720/24-087 to a constant six. We used LightGBM to model the data, a \npowerful gradient booster that combines accuracy with \nefficiency. Additionally, the hierarchical-based processing \nof the LightGBM model allows for feature importance to be \nestimated (14). The relative importance of each feature was \ncalculated with word count, paragraph count, and pause \nfrequencies being among the most important (Figure 2) . We \nalso graphed and analyzed the types of error (Figure 3) . The \nmodel was fairly even in the errors it made, overestimating \nand underestimating the score an essay received fairly \nequally. Additionally, barring any outliers, the model was at \nmost off by two points for a grade when assessing a given \nessay, indicating that there was generally a strong agreement \nbetween the predictions and the official scores.  To fully understand how the performance of this model \ncompares to LLM based methods and to address our original \nhypothesis, we compared our results to prior research that \nused LLMs to assess the quality of essay writing (16). We \nspecifically examine Table 3 of the paper as it features the \nQWK scores for two tasks most related to our own (automatic \nessay grading with a score range of 1\u20136). Our model scores \nan average QWK of 0.7018 (Figure 4) . In comparison, most of \nthe LLMs featured in the paper achieve under 0.7 for the first \nexperiment and above 0.775 for the second experiment. \nDISCUSSION\n In this paper, we highlight a different way of approaching \nnatural language tasks, one that doesn\u2019t involve the language \nFigure 2: LightGBM\u2019s feature importance. The importance of each feature to the model trained on a specific data split, where a higher \nnumerical value means that a feature is relatively more important than another. This means that a feature. The number of words and paragraphs \nare shown to be most important to the model.\nFigure 1: LightGBM performance on evaluating student essays based on RMSE and QWK. This graph shows the results the model \nobtained for each of the 5 data splits, including its quantitative performance and the time it took to train. An average RMSE of 0.6624 was \nobtained taking 0.3774 seconds on average.\n04 OCT 2024  |  VOL 7  |  4\nJournal of Emerging Investigators  \u2022  www.emerginginvestigators.org\nhttps://doi.org/10.59720/24-087 itself. In the last few years, attention-based methods have \ntaken over the field of NLP. While these methods have \ndrastically improved on the results obtained by older methods, \nthey have also increased the run time and resources required\nto model such data. Right now, the norm appears to be fine-\ntuning models with billions of parameters to achieve strong \nresults on down-stream tasks. Specifically, the transformers \nwe compared to were trained with a Nvidia RTX8000 GPU \nand a majority of the models used had well over a million \nparameters. These results reveal two very important \ncomparisons with our own experiment. Firstly, our own \ngradient booster is able to achieve a very similar performance \non scoring essays as LLMs, as can be seen with the numbers \nabove. Secondly, we can see that without the use of a GPU, \nour own model requires significantly less time or computing \npower due to the significantly smaller nature of the model. \nThis supports our original hypothesis as our gradient booster \nusing keystroke features performs at a level comparable to \ntransformer based LLMs while using less time and computing \npower. \n Keystroke log data also gives us insight that we are not \nable to observe normally with raw textual data. Although \nincorporating keystroke information with the actual text \ndata could further improve results, we found only using the \nkeystroke log data was not only effective but also highly \nefficient. Gradient boosters are extremely quick and precise when handling datasets with a relatively low number of \nfeatures (around 40 used here). Additionally, the writing \nprocess itself offers a new way to view and process textual \ndata. Rather than the words that ended up on the final paper, \nthe words that were deleted, sentences that were edited, and \nother features that are missing from the final product could be \nused as inputs to simpler machine learning models. Machine \nlearning models have been used in the past for processing \ntext, typically paired with embedding models like term \nfrequency-inverse document frequency (TFIDF) vectorizers, \nGLOVE vectors, and FastText amongst many others (17, 18). \nAlthough these methods are often more cost-effective than \ntransformer-based methods, they usually perform worse. \nAdditionally, with almost all of these methods, the textual data \nis transformed into a feature space with a high dimensionality, \nresulting in a time-consuming training time even for smaller \nmachine learning models.\n However, using gradient boosters with keystroke log data, \nwe were able to exploit the writing process to represent an \nessay with a significantly smaller feature set. Additionally, a \nstrong normalized RMSE demonstrates the strong capability \nof gradient boosters trained on keystroke log features. Further \ntask-related optimization could drop this loss further, such as \nmodel fine-tuning or more feature engineering. \n There were some limitations and shortcomings to note \nwhile interpreting these results. The first major limitation \nregards the process of collecting keystroke log data. Unlike \nraw textual data, which can easily be recorded in the form \nof a response or scraped off the internet, keystroke log \ndata requires software to collect the data and permissions \nfrom users. This is a far more complicated process for data \ncollection compared to traditional methods, which could \nmake it difficult to use keystroke log data in certain tasks. \nAdditionally, performing inference with keystroke log data \nwould require keystroke log software to be active while a new \nuser is typing. The data collection phase is more difficult for \nkeystroke log data compared to raw textual data and limits the \nusability of the methods provided here.\n Another limitation faced in this paper is a lack of reliable \ncomparison between keystroke log methods and state of \nthe art transformer-based methods. Due to the anonymous \nFigure 4: Sample time series displaying the number of keystrokes over time. The number of keystrokes in each 60 second time slice \n(sequential segments of 60 second time spans where an index corresponds to the position of the segment) for a single participant while they \nwere writing (blue). It also shows the line of best fit used to determine slope degree (for 30 and 60 second time slices) (orange) and some \nnoticeable local maxes and mins (red).\nFigure 3: Prediction residuals. The above figure shows a histogram \nof the differences between the predictions made and the true targets.\n04 OCT 2024  |  VOL 7  |  5\nJournal of Emerging Investigators  \u2022  www.emerginginvestigators.org\nhttps://doi.org/10.59720/24-087 nature of the data used in this paper, it is not possible to see \nhow a transformer model would have scored on the original \ntext data nor is it possible to see the performance of a hybrid \nmodel. While we did attempt to compare the performance of \nLLMs compared to gradient boosters, this comparison was \nnot made on the same set of data, so it instead serves as a \nrough estimate rather than a completely precise measure.\n Using keystroke log data to model textual inputs is a \nless explored part of NLP and we presented ideas intended \nto serve as a baseline. Several changes or additions could \nimprove the results. These revisions can be as simple as \noptimizing the hyper-parameters of the model and ensembling \nor stacking several models, or as complicated as engineering \nnew features from the time-series component of the data. \nAnother important development could be the creation of \na non-anonymized keystroke log dataset. This would allow \nresearchers the ability to compare the strength of models \nusing keystroke log data to the strength of models using \ntextual data. This could show the validity of keystroke-based \nmethods as well as highlighting the trade-off in efficiency \nversus performance. Additionally, this data could be used to \nexplore hybrid methods which utilize both the final product \nand the process of getting to the final product. Such a model \nwould be able to get a strong variety of features, which could \nallow for even stronger results. Another potentially interesting \nform of research with keystroke log data is a longitudinal \nanalysis. Uncovering changes in the way people type as a \nfunction of time could be both interesting and insightful.\n While we focused on optimizing time complexity, optimizing \nperformance could also be achieved. The easiest and possibly \nmost effective way for achieving stronger results is to adjust \nthe parameters of the model. For example, increasing the \nnumbe",
      "openai_available": true,
      "pages": 7,
      "processed": true,
      "recommendations": [
        "Consider including evidence of awards, recognitions, or high rankings in the field which validate your exceptional ability",
        "Include clear examples of your work's influence or contributions to others in the field",
        "Outline how exactly your work can advance U.S interests in O1 application to build a stronger case",
        "Expand on the practical implications of your research and its potential impacts on society",
        "Select more comprehensive expert letters that clearly delineate your key achievements and standing within your field to augment your application",
        "Try to simplify technical information for a wider audience to understand your contributions better."
      ],
      "strengths": [
        "You're highly knowledgeable about the subject matter",
        "Your document is well-organized and comprehensive, with the inclusion of detailed analyses and interpretations of findings",
        "The data and research findings presented in the document are robust and critical to the O1 application",
        "You've demonstrated clear potential in improving the modeling of textual data using keystroke based features",
        "The document contains several high-quality research studies to support your claims",
        "The time and resource efficiency aspects of your proposition strengthen your application."
      ],
      "summary": "Strengths: \n[SEP] You're highly knowledgeable about the subject matter\n[SEP] Your document is well-organized and comprehensive, with the inclusion of detailed analyses and interpretations of findings\n[SEP] The data and research findings presented in the document are robust and critical to the O1 application\n[SEP] You've demonstrated clear potential in improving the modeling of textual data using keystroke based features\n[SEP] The document contains several high-quality research studies to support your claims\n[SEP] The time and resource efficiency aspects of your proposition strengthen your application.\n\nWeaknesses: \n[SEP] The level of complexity in some sections of your document may be difficult for non-expert reviewers to understand\n[SEP] The application lacks concrete evidence of the exceptional ability in your field\n[SEP] It is slightly unclear how exactly your work will advance U.S. interests\n[SEP] You've provided limited evidence of peer recognition. Beyond your publication, there is not much indication of industry recognition of your work.\n[SEP] It is not evident how your work has influenced other work in your field.\n\nRecommendations: \n[SEP] Consider including evidence of awards, recognitions, or high rankings in the field which validate your exceptional ability\n[SEP] Include clear examples of your work's influence or contributions to others in the field \n[SEP] Outline how exactly your work can advance U.S interests in O1 application to build a stronger case\n[SEP] Expand on the practical implications of your research and its potential impacts on society \n[SEP] Select more comprehensive expert letters that clearly delineate your key achievements and standing within your field to augment your application\n[SEP] Try to simplify technical information for a wider audience to understand your contributions better.",
      "text_preview": "04 OCT 2024  |  VOL 7  |  1\nJournal of Emerging Investigators  \u2022  www.emerginginvestigators.org\nArticleLLM that was built on BERT, is currently ranked seventh for \nthe General Language Understanding Evaluation (GLUE) \nbenchmark (3,4). The GLUE benchmark is a large dataset \nthat creates several fundamental tasks of natural language \nprocessing (NLP) including question answering and text \nclassification, amongst others. The base version of the BERT \nmodel comprises 86 million total parameters while the large \nversion has over 300 million parameters. To be able to fine-tune \nsuch a model on a task, a GPU and a considerable amount of \ntime is required. Additionally, achieving optimal performance \nrequires hyper parameter tweaking, such as training for more/\nless epochs (training iterations) and increasing/decreasing \nthe learning rate (how aggressive the training approach is), \nwhich further the total time complexity. Traditional methods \nattempt to use classical machine learning algorithm",
      "weaknesses": [
        "The level of complexity in some sections of your document may be difficult for non-expert reviewers to understand",
        "The application lacks concrete evidence of the exceptional ability in your field",
        "It is slightly unclear how exactly your work will advance U.S. interests",
        "You've provided limited evidence of peer recognition. Beyond your publication, there is not much indication of industry recognition of your work.",
        "It is not evident how your work has influenced other work in your field."
      ]
    },
    "resume": {
      "extracted_text": "Ryan\n \nBarretto\n \nryanbarretto70@gmail.com\n \n|\n \n(847)\n \n246-2461\n \n|\n \nChicago,\n \nIL\n  \n \n \nEDUCATION\n \n \n \nWilliam\n \nFremd\n \nHigh\n \nSchool\n \n \n \n \n                     \n \n               \n \n   \nMay\n \n2025\n \n \nClass\n \nof\n \n2025\n \n \n                           \n \n\u25aa\n \nUnweighted\n \nGPA\n \n\u2013\n \n4.0\n \n\u25aa\n \nWeighted\n \nGPA\n \n\u2013\n \n5.37\n \n\u25aa\n \nAP\n \nClasses\n \n\u2013\n \n14\n \n \nWORK\n \nEXPERIENCE\n \n \nBizzflo\n \nBusiness\n \nManagement\n \nSoftware\n \n \n \n \n \n                       \n \n   \nJuly\n \n2024\n \nMachine\n \nLearning\n \nIntern\n \n                                                   \n \n \n \n \n   \n \n \n     \n \n       \n \nRemote\n \n\u25aa\n \nUsed\n \nArtificial\n \nIntelligence\n \nbased\n \ntools\n \nto\n \nautomate\n \nvideo\n \ncreation\n \ndetailing\n \ninstructional\n \nbased\n \nmaterials\n \n \nThe\n \nLearning\n \nAgency\n \nLab\n \n \n \n \n \n                       \n \n                 \nMay\n \n2023\n \n\u2013\n \nAug\n \n2023\n \nArtificial\n \nIntelligence\n \nIntern\n \n                                                   \n \n \n \n \n   \n \n \n     \n \n       \n \nRemote\n \n\u25aa\n \nFocus\n \non\n \nNatural\n \nLanguage\n \nProcessing\n \n(NLP),\n \ncompleting\n \ntasks\n \nsuch\n \nas\n \nautomatic\n \nessay\n \ndetection,\n \npersonally\n \nidentifiable\n \ninformation\n \n(PII)\n \ndetection,\n \nautomatic\n \ngrammar\n \ncorrection,\n \nand\n \nkeystroke\n \nlog\n \nanalysis.\n \n \n\u25aa\n \nUtilized\n \nstate\n \nof\n \nthe\n \nart\n \ntechniques\n \nin\n \nthe\n \nfield\n \nof\n \nNLP\n \ninvolving\n \nattention-based\n \ntransformer\n \nmodels\n \nas\n \nwell\n \nas\n \nmethods\n \nfrom\n \nmachine\n \nlearning\n \nincluding\n \ngradient\n \nboosting.\n \nKaggle\n \n \n \n \n \n \n \n \n \n \n \n   \nJanuary\n \n2021\n \n\u2013\n \nPresent\n \nPrivate\n \nContributor\n \n                                                               \n \n \n   \n \n \n        \n \n\u25aa\n \nPublished\n \nnumerous\n \ncode\n \nnotebooks\n \nwhich\n \nearned\n \nmedals\n \nincluding\n \n1\n \ngold,\n \n5\n \nsilver,\n \nand\n \n3\n \nbronze\n \nand\n \na\n \npeak\n \nglobal\n \nnotebooks\n \nranking\n \nof\n \n656\n \nin\n \nthe\n \nworld.\n \n \n\u25aa\n \nParticipated\n \nin\n \nseveral\n \ncompetitions\n \nand\n \ndiscussion\n \nthreads\n \npeaking\n \nat\n \n1600th\n \nin\n \nthe\n \nworld\n \nfor\n \ncompetitions\n \nand\n \n800th\n \nfor\n \ndiscussions.\n \n \nAWARDS\n \n \n\u25aa\n \nUS\n \nChess\n \nCandidate\n \nMaster\n \no\n \n54th\n \nhighest\n \nrated\n \nplayer\n \nin\n \nthe\n \nUS\n \nfor\n \nage\n \n16\n \no\n \nUSCF\n \nrating\n \n\u2013\n \n2111\n \n\u25aa\n \nSilver\n \nmedalist\n \nin\n \nInvitro\n \nCell\n \nResearch\n \nCompetition\n \n(101st\n \nout\n \nof\n \n6430).\n \n \no\n \nPredicted\n \nseveral\n \nhealth\n \nrelated\n \nconditions\n \ngiven\n \nanonymized\n \nmedical\n \ndata.\n \nUtilized\n \ngradient\n \nboosters\n \nand\n \ntabular\n \nfeature\n \nextraction\n \nfor\n \nthe\n \nfinal\n \nsolution.\n \n\u25aa\n \nBronze\n \nmedalist\n \nin\n \nGeorgia\n \nState\n \nUniversity\n \nCompetition\n \n(78th\n \nout\n \nof\n \n1557)\n \no\n \nClassified\n \nargumentative\n \nelements\n \nin\n \nstudent\n \nwriting\n \nusing\n \ntransformer\n \nmodels,\n \nlike\n \nDeBERTa\n \nand\n \noptimization\n \nstrategies\n \nlike\n \nAdamW\n \nfor\n \nmultivariate\n \nprediction.\n \n\u25aa\n \nUSACO\n \nSilver\n \n(United\n \nStates\n \nof\n \nAmerica\n \nComputing\n \nOlympiad)\n \n \no\n \nSolved\n \nalgorithm\n \nbased\n \nproblems\n \ninvolving\n \ntrees,\n \ngraphs,\n \ngreedy\n \nalgorithms\n \nand\n \ndynamic\n \nprogramming\n \n\u25aa\n \nScholastic\n \nWriting\n \nSilver\n \nKey\n \no\n \nNon-fiction\n \nanalytical\n \nessay\n \non\n \nthe\n \nclose\n \nrelationship\n \nbetween\n \nAI\n \nand\n \nchess\n \nover\n \nthe\n \ncourse\n \nof\n \nthe\n \nlast\n \nfew\n \ndecades.\n \n \n\u25aa\n \nM3\n \nMath\n \nModeling\n \nHonorable\n \nMention\n \no\n \nCo-wrote\n \na\n \npaper\n \nthat\n \nused\n \nmathematical\n \nmodels\n \nlike\n \nARIMA,\n \nmarkov\n \nchains,\n \nlogistic\n \nregression\n \nto\n \nsolve\n \nhousing\n \nrelated\n \nissues\n \n \nLEADERSHIP\n \nEXPERIENCE\n \n \n\u25aa\n \nCofounded\n \nhigh\n \nschool\u2019s\n \nmachine\n \nlearning\n \nand\n \nartificial\n \nintelligence\n \nclub\n \n \n\u25aa\n \nChess\n \nclub\n \nexec\n \nboard\n \n(3\n \nyears),\n \nhelped\n \nincoming\n \nmembers\n \nimprove\n \ntheir\n \ngame\n \nand\n \nstrengthen\n \nthe\n \nteam.\n \nLed\n \nteam\n \nto\n \nthird\n \nplace\n \nstate\n \nfinish\n \n(2025)\n \n\u25aa\n \nLed\n \nAI\n \nworkshops\n \nfor\n \ndistrict\n \nhackathons\n \n \nSKILLS\n \nAND\n \nINTERESTS\n \n\u25aa\n \nSeveral\n \nyears\n \nof\n \nexperience\n \nwith\n \nPython,\n \nand\n \nspecifically\n \nframeworks\n \nsuch\n \nas\n \nPytorch,\n \nTensorflow,\n \nand\n \nTransformers\n \n\u25aa\n \nExperience\n \nwith\n \nC++\n \nand\n \nalgorithms\n \n \n\u25aa\n \nSeveral\n \nyears\n \nusing\n \nand\n \nimplementing\n \nmachine\n \nlearning\n \nmodels\n \nlike\n \ngradient\n \nboosters,\n \nsupport\n \nvector\n \nmachines,\n \netc\u2026\n \n\u25aa\n \nSeveral\n \nyears\n \nusing\n \nand\n \nimplementing\n \ndeep\n \nlearning\n \nmodels\n \nlike\n \ntransformers,\n \nconvolutional\n \nneural\n \nnetworks,\n \netc\u2026\n \n \nPUBLICATIONS\n \n\u25aa\n \nGradient\n \nBoosting\n \nwith\n \nTemporal\n \nFeature\n \nExtraction\n \nfor\n \nModeling\n \nKeystroke\n \nLog\n \nData\n \n\u2013\n \nPublished\n \non\n \nfigshare\n \n\u25aa\n \nHow\n \nI\n \nJoined\n \nA\n \nKaggle\n \nCompetition\n \nAs\n \nA\n \nHigh\n \nSchooler\n \n\u2013\n \nPublished\n \non\n \nthe\n \nLearning\n \nAgency\n \nLab\n \n \nINDEPENDENT\n \nPROJECTS\n \n\u25aa\n \nAI\n \nBased\n \nMarket\n \nForecaster\n \nand\n \nTrader\n \no\n \nUsed\n \ntime\n \nseries\n \ntransformer\n \nmodels\n \nto\n \nmake\n \npredictions\n \non\n \nthe\n \nmarket\n \nand\n \nthen\n \nselect\n \nstocks\n \nbelieved\n \nto\n \nbe\n \nhighly\n \nprofitable\n \n\u25aa\n \nAutoKeystroke\n \nLibrary\n \no\n \nCreated\n \na\n \nlibrary\n \nreleased\n \non\n \npython\n \nwhich\n \nallows\n \nusers\n \nto\n \nautomatically\n \nprocess\n \nkeystroke\n \nlog\n \ndata\n \nand\n \nmodel\n \nthe\n \ndata\n \n ",
      "openai_available": true,
      "pages": 2,
      "processed": true,
      "recommendations": [
        "Try to gain more work experience or internships in your field, especially with well-known or international companies.",
        "Try to participate in international events or cross-cultural experiences to demonstrate versatility and adaptability.",
        "Provide more details about your role and achievements in team projects or during your leadership in clubs and workshops.",
        "Flesh out the description of your leadership roles to better display your management and team leading skills.",
        "Consider including references or referees who can validate your skills, experiences and the claims you make in your application."
      ],
      "strengths": [
        "Excellent academic background with high Unweighted and Weighted GPA and substantial number of AP classes.",
        "Demonstrated work experience, notably in Machine Learning and Artificial Intelligence.",
        "Broad range of awards and commendations demonstrating academic and professional prowess.",
        "Strong record of leadership in clubs and workshops related to the field of work.",
        "Proficient with industry-relevant tools, languages and frameworks like Python, Pytorch, Tensorflow, and Transformers.",
        "Applicant has published works and completed independent projects showcasing initiative and deep understanding of the field."
      ],
      "summary": "Strengths: \n\n[SEP] Excellent academic background with high Unweighted and Weighted GPA and substantial number of AP classes. [SEP] Demonstrated work experience, notably in Machine Learning and Artificial Intelligence. [SEP] Broad range of awards and commendations demonstrating academic and professional prowess. [SEP] Strong record of leadership in clubs and workshops related to the field of work. [SEP] Proficient with industry-relevant tools, languages and frameworks like Python, Pytorch, Tensorflow, and Transformers. [SEP] Applicant has published works and completed independent projects showcasing initiative and deep understanding of the field.\n\nWeaknesses: \n\n[SEP] Limited work experience due to recent completion of education. [SEP] Lack of international or cross-cultural experiences which may improve adaptability and versatility. [SEP] Limited details on individual contribution in teamwork settings. [SEP] The role and impact of leadership experiences are not well defined. \n\nRecommendations: \n\n[SEP] Try to gain more work experience or internships in your field, especially with well-known or international companies. [SEP] Try to participate in international events or cross-cultural experiences to demonstrate versatility and adaptability. [SEP] Provide more details about your role and achievements in team projects or during your leadership in clubs and workshops. [SEP] Flesh out the description of your leadership roles to better display your management and team leading skills. [SEP] Consider including references or referees who can validate your skills, experiences and the claims you make in your application.",
      "text_preview": "Ryan\n \nBarretto\n \nryanbarretto70@gmail.com\n \n|\n \n(847)\n \n246-2461\n \n|\n \nChicago,\n \nIL\n  \n \n \nEDUCATION\n \n \n \nWilliam\n \nFremd\n \nHigh\n \nSchool\n \n \n \n \n                     \n \n               \n \n   \nMay\n \n2025\n \n \nClass\n \nof\n \n2025\n \n \n                           \n \n\u25aa\n \nUnweighted\n \nGPA\n \n\u2013\n \n4.0\n \n\u25aa\n \nWeighted\n \nGPA\n \n\u2013\n \n5.37\n \n\u25aa\n \nAP\n \nClasses\n \n\u2013\n \n14\n \n \nWORK\n \nEXPERIENCE\n \n \nBizzflo\n \nBusiness\n \nManagement\n \nSoftware\n \n \n \n \n \n                       \n \n   \nJuly\n \n2024\n \nMachine\n \nLearning\n \nIntern\n \n                                                   \n \n \n \n \n   \n \n \n     \n \n       \n \nRemote\n \n\u25aa\n \nUsed\n \nArtificial\n \nIntelligence\n \nbased\n \ntools\n \nto\n \nautomate\n \nvideo\n \ncreation\n \ndetailing\n \ninstructional\n \nbased\n \nmaterials\n \n \nThe\n \nLearning\n \nAgency\n \nLab\n \n \n \n \n \n                       \n \n                 \nMay\n \n2023\n \n\u2013\n \nAug\n \n2023\n \nArtificial\n \nIntelligence\n \nIntern\n \n                                                   \n \n \n \n \n   \n \n \n     \n \n       \n \nRemote\n \n\u25aa\n \nFocus\n \no",
      "weaknesses": [
        "Limited work experience due to recent completion of education.",
        "Lack of international or cross-cultural experiences which may improve adaptability and versatility.",
        "Limited details on individual contribution in teamwork settings.",
        "The role and impact of leadership experiences are not well defined."
      ]
    }
  },
  "additional_info": {
    "address": "Cannery Row, Monterey, CA 93940, USA",
    "additional_comments": ""
  }
}
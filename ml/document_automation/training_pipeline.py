#!/usr/bin/env python3
"""
Training Pipeline for Document Automation Model.

This module provides utilities for training the document filling model
using synthetic data generated by the previous pipeline stages.
"""

import os
import json
import logging
import time
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from typing import Dict, List, Any, Optional, Union, Tuple
from torch.utils.data import DataLoader, Dataset
from torch.optim import Adam, AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torch.nn.functional as F

# Import local modules
from .data_loader import SyntheticDataLoader, DocumentPairDataset
from .model_architecture import (
    DocumentFillingModel, 
    create_model, 
    save_model, 
    load_model
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Root directory for models
ROOT_DIR = Path(__file__).resolve().parent.parent.parent
MODEL_DIR = ROOT_DIR / "data" / "models"


class Tokenizer:
    """Simple tokenizer for field names and values."""
    
    def __init__(
        self,
        vocab_file: Optional[Union[str, Path]] = None,
        max_vocab_size: int = 10000,
    ):
        """Initialize the tokenizer.
        
        Args:
            vocab_file: Optional path to a vocabulary file
            max_vocab_size: Maximum vocabulary size
        """
        self.vocab = {"<pad>": 0, "<unk>": 1, "<start>": 2, "<end>": 3}
        self.reverse_vocab = {0: "<pad>", 1: "<unk>", 2: "<start>", 3: "<end>"}
        self.max_vocab_size = max_vocab_size
        
        # Load vocabulary if provided
        if vocab_file:
            self.load_vocab(vocab_file)
    
    def build_vocab(self, texts: List[str]) -> Dict[str, int]:
        """Build vocabulary from a list of texts.
        
        Args:
            texts: List of text strings
            
        Returns:
            Dictionary mapping tokens to indices
        """
        token_counts = {}
        
        # Count tokens
        for text in texts:
            for token in self._tokenize(text):
                if token not in token_counts:
                    token_counts[token] = 0
                token_counts[token] += 1
        
        # Sort by frequency
        sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)
        
        # Add to vocabulary (up to max size)
        current_idx = len(self.vocab)
        for token, count in sorted_tokens:
            if token not in self.vocab and current_idx < self.max_vocab_size:
                self.vocab[token] = current_idx
                self.reverse_vocab[current_idx] = token
                current_idx += 1
        
        logger.info(f"Vocabulary size: {len(self.vocab)}")
        
        return self.vocab
    
    def save_vocab(self, path: Union[str, Path]):
        """Save vocabulary to a file.
        
        Args:
            path: Path to save the vocabulary to
        """
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(path, "w", encoding="utf-8") as f:
            json.dump(self.vocab, f, indent=2)
        
        logger.info(f"Vocabulary saved to {path}")
    
    def load_vocab(self, path: Union[str, Path]):
        """Load vocabulary from a file.
        
        Args:
            path: Path to load the vocabulary from
        """
        path = Path(path)
        
        if not path.exists():
            raise FileNotFoundError(f"Vocabulary file not found: {path}")
        
        with open(path, "r", encoding="utf-8") as f:
            self.vocab = json.load(f)
        
        # Build reverse vocabulary
        self.reverse_vocab = {idx: token for token, idx in self.vocab.items()}
        
        logger.info(f"Loaded vocabulary with {len(self.vocab)} tokens from {path}")
    
    def _tokenize(self, text: str) -> List[str]:
        """Tokenize a text string.
        
        Args:
            text: Text string to tokenize
            
        Returns:
            List of tokens
        """
        # Simple whitespace tokenization for now
        # In a real-world scenario, we would use a more sophisticated tokenizer
        return text.split()
    
    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:
        """Encode a text string as a list of token indices.
        
        Args:
            text: Text string to encode
            max_length: Maximum sequence length (with padding)
            
        Returns:
            List of token indices
        """
        tokens = self._tokenize(text)
        
        # Add start and end tokens
        tokens = ["<start>"] + tokens + ["<end>"]
        
        # Convert to indices
        indices = [self.vocab.get(token, self.vocab["<unk>"]) for token in tokens]
        
        # Apply sequence length limit if specified
        if max_length:
            indices = indices[:max_length]
            
            # Pad if necessary
            if len(indices) < max_length:
                indices = indices + [self.vocab["<pad>"]] * (max_length - len(indices))
        
        return indices
    
    def decode(self, indices: List[int]) -> str:
        """Decode a list of token indices to a text string.
        
        Args:
            indices: List of token indices
            
        Returns:
            Decoded text string
        """
        # Convert indices to tokens
        tokens = [self.reverse_vocab.get(idx, "<unk>") for idx in indices]
        
        # Remove special tokens
        tokens = [token for token in tokens if token not in ["<pad>", "<start>", "<end>"]]
        
        # Join tokens
        return " ".join(tokens)


class FormFillingTrainer:
    """Trainer for the document filling model."""
    
    def __init__(
        self,
        model: DocumentFillingModel,
        tokenizer: Tokenizer,
        optimizer: torch.optim.Optimizer,
        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
        device: str = "cuda" if torch.cuda.is_available() else "cpu"
    ):
        """Initialize the trainer.
        
        Args:
            model: Document filling model
            tokenizer: Tokenizer for encoding and decoding
            optimizer: Optimizer for training
            scheduler: Optional learning rate scheduler
            device: Device to use for training (cuda or cpu)
        """
        self.model = model
        self.tokenizer = tokenizer
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        
        # Move model to device
        self.model.to(self.device)
        
        # Track metrics
        self.train_losses = []
        self.val_losses = []
        self.best_val_loss = float("inf")
    
    def train_epoch(
        self,
        train_loader: DataLoader,
        epoch: int,
        clip_grad_norm: float = 1.0
    ) -> float:
        """Train the model for one epoch.
        
        Args:
            train_loader: DataLoader for training data
            epoch: Current epoch number
            clip_grad_norm: Maximum gradient norm
            
        Returns:
            Average training loss
        """
        self.model.train()
        total_loss = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch}")
        
        for batch_idx, batch in enumerate(progress_bar):
            # Move batch to device
            field_names = batch["field_names"].to(self.device)
            field_values = batch["field_values"].to(self.device)
            field_types = batch["field_types"].to(self.device)
            
            # Clear gradients
            self.optimizer.zero_grad()
            
            # Forward pass
            value_logits, type_logits = self.model(field_names, field_values)
            
            # Calculate loss for value prediction
            # Ignore padding tokens
            value_loss = F.cross_entropy(
                value_logits.view(-1, value_logits.size(-1)),
                field_values.view(-1),
                ignore_index=self.tokenizer.vocab["<pad>"]
            )
            
            # Calculate loss for field type prediction
            type_loss = F.cross_entropy(type_logits, field_types)
            
            # Combined loss
            loss = value_loss + 0.1 * type_loss
            
            # Backward pass
            loss.backward()
            
            # Clip gradients
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_grad_norm)
            
            # Update weights
            self.optimizer.step()
            
            # Update metrics
            total_loss += loss.item()
            
            # Update progress bar
            progress_bar.set_postfix(
                loss=loss.item(),
                value_loss=value_loss.item(),
                type_loss=type_loss.item()
            )
        
        # Calculate average loss
        avg_loss = total_loss / len(train_loader)
        self.train_losses.append(avg_loss)
        
        return avg_loss
    
    def validate(self, val_loader: DataLoader) -> float:
        """Validate the model.
        
        Args:
            val_loader: DataLoader for validation data
            
        Returns:
            Average validation loss
        """
        self.model.eval()
        total_loss = 0
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                # Move batch to device
                field_names = batch["field_names"].to(self.device)
                field_values = batch["field_values"].to(self.device)
                field_types = batch["field_types"].to(self.device)
                
                # Forward pass
                value_logits, type_logits = self.model(field_names, field_values)
                
                # Calculate loss for value prediction
                value_loss = F.cross_entropy(
                    value_logits.view(-1, value_logits.size(-1)),
                    field_values.view(-1),
                    ignore_index=self.tokenizer.vocab["<pad>"]
                )
                
                # Calculate loss for field type prediction
                type_loss = F.cross_entropy(type_logits, field_types)
                
                # Combined loss
                loss = value_loss + 0.1 * type_loss
                
                # Update metrics
                total_loss += loss.item()
        
        # Calculate average loss
        avg_loss = total_loss / len(val_loader)
        self.val_losses.append(avg_loss)
        
        # Update learning rate scheduler if provided
        if self.scheduler:
            self.scheduler.step(avg_loss)
        
        # Check if this is the best model so far
        if avg_loss < self.best_val_loss:
            self.best_val_loss = avg_loss
            return avg_loss, True
        
        return avg_loss, False
    
    def train(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        epochs: int,
        model_dir: Union[str, Path],
        save_every: int = 5,
        early_stopping: int = 10
    ):
        """Train the model.
        
        Args:
            train_loader: DataLoader for training data
            val_loader: DataLoader for validation data
            epochs: Number of epochs to train
            model_dir: Directory to save models
            save_every: Save model every N epochs
            early_stopping: Stop training if validation loss doesn't improve for N epochs
        """
        model_dir = Path(model_dir)
        model_dir.mkdir(parents=True, exist_ok=True)
        
        # Track best model and patience for early stopping
        best_epoch = 0
        patience_counter = 0
        
        # Training loop
        for epoch in range(1, epochs + 1):
            # Train for one epoch
            train_loss = self.train_epoch(train_loader, epoch)
            
            # Validate
            val_loss, is_best = self.validate(val_loader)
            
            # Log results
            logger.info(f"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}")
            
            # Save model if it's the best so far
            if is_best:
                best_epoch = epoch
                patience_counter = 0
                save_model(self.model, model_dir / "best_model.pt")
                logger.info(f"New best model saved (Epoch {epoch})")
            else:
                patience_counter += 1
            
            # Save checkpoint if needed
            if epoch % save_every == 0:
                save_model(self.model, model_dir / f"model_epoch_{epoch}.pt")
                logger.info(f"Checkpoint saved at epoch {epoch}")
            
            # Check for early stopping
            if patience_counter >= early_stopping:
                logger.info(f"Early stopping triggered after {patience_counter} epochs without improvement")
                break
        
        # Final log
        logger.info(f"Training completed. Best model at epoch {best_epoch} with val loss {self.best_val_loss:.4f}")
        
        # Load best model for evaluation
        best_model_path = model_dir / "best_model.pt"
        if best_model_path.exists():
            self.model.load_state_dict(torch.load(best_model_path))
            logger.info(f"Loaded best model from {best_model_path}")
    
    def predict(self, field_names: List[str], max_length: int = 128) -> List[str]:
        """Predict field values for a list of field names.
        
        Args:
            field_names: List of field names
            max_length: Maximum sequence length for generated values
            
        Returns:
            List of predicted field values
        """
        self.model.eval()
        
        # Encode field names
        encoded_names = [self.tokenizer.encode(name) for name in field_names]
        
        # Create input tensor
        field_name_tensor = torch.tensor(encoded_names, dtype=torch.long).to(self.device)
        
        # Generate predictions
        with torch.no_grad():
            value_logits, type_logits = self.model(field_name_tensor)
            
            # Get predicted values
            predicted_indices = torch.argmax(value_logits, dim=-1).cpu().numpy()
            
            # Decode predictions
            predicted_values = [
                self.tokenizer.decode(indices)
                for indices in predicted_indices
            ]
        
        return predicted_values


def prepare_training_data(
    data_dir: Optional[Union[str, Path]] = None,
    doc_types: Optional[List[str]] = None,
    batch_size: int = 32,
    max_seq_length: int = 128
):
    """Prepare training and validation data.
    
    Args:
        data_dir: Directory containing synthetic data
        doc_types: List of document types to include
        batch_size: Batch size for dataloaders
        max_seq_length: Maximum sequence length
        
    Returns:
        Train dataloader, validation dataloader, and tokenizer
    """
    # Load synthetic data
    data_loader = SyntheticDataLoader(
        data_dir=data_dir,
        doc_types=doc_types
    )
    
    # Load data
    data_loader.load_data()
    
    # Create datasets
    train_dataset = DocumentPairDataset(
        data_loader=data_loader,
        doc_type="o1",  # Focus on O-1 visa forms
        split="train"
    )
    
    val_dataset = DocumentPairDataset(
        data_loader=data_loader,
        doc_type="o1",
        split="val"
    )
    
    # Create tokenizer
    tokenizer = Tokenizer(max_vocab_size=10000)
    
    # Collect all texts for vocabulary building
    all_texts = []
    
    # Add field names from training data
    train_df = train_dataset.to_dataframe()
    all_texts.extend(train_df["field_name"].tolist())
    all_texts.extend(train_df["field_value"].tolist())
    
    # Build vocabulary
    tokenizer.build_vocab(all_texts)
    
    # Save vocabulary
    tokenizer.save_vocab(Path(MODEL_DIR) / "vocab.json")
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    return train_loader, val_loader, tokenizer


def train_model(
    data_dir: Optional[Union[str, Path]] = None,
    model_dir: Optional[Union[str, Path]] = None,
    doc_types: Optional[List[str]] = None,
    batch_size: int = 32,
    max_seq_length: int = 128,
    embedding_dim: int = 256,
    hidden_dim: int = 512,
    encoder_layers: int = 4,
    decoder_layers: int = 4,
    num_heads: int = 8,
    dropout: float = 0.1,
    learning_rate: float = 1e-4,
    weight_decay: float = 1e-5,
    epochs: int = 50,
    save_every: int = 5,
    early_stopping: int = 10,
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
):
    """Train the document filling model.
    
    Args:
        data_dir: Directory containing synthetic data
        model_dir: Directory to save models
        doc_types: List of document types to include
        batch_size: Batch size for training
        max_seq_length: Maximum sequence length
        embedding_dim: Dimension of token embeddings
        hidden_dim: Dimension of hidden layers
        encoder_layers: Number of transformer encoder layers
        decoder_layers: Number of transformer decoder layers
        num_heads: Number of attention heads
        dropout: Dropout probability
        learning_rate: Learning rate for optimizer
        weight_decay: Weight decay for optimizer
        epochs: Number of epochs to train
        save_every: Save model every N epochs
        early_stopping: Stop training if validation loss doesn't improve for N epochs
        device: Device to use for training (cuda or cpu)
    """
    # Set directories
    if data_dir is None:
        data_dir = ROOT_DIR / "data" / "training" / "synthetic"
    
    if model_dir is None:
        model_dir = MODEL_DIR
    
    # Create model directory
    model_dir = Path(model_dir)
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # Prepare training data
    logger.info("Preparing training data...")
    train_loader, val_loader, tokenizer = prepare_training_data(
        data_dir=data_dir,
        doc_types=doc_types,
        batch_size=batch_size,
        max_seq_length=max_seq_length
    )
    
    # Create model
    logger.info("Creating model...")
    model = create_model(
        vocab_size=len(tokenizer.vocab),
        embedding_dim=embedding_dim,
        hidden_dim=hidden_dim,
        encoder_layers=encoder_layers,
        decoder_layers=decoder_layers,
        num_heads=num_heads,
        max_seq_length=max_seq_length,
        dropout=dropout
    )
    
    # Create optimizer
    optimizer = AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay
    )
    
    # Create learning rate scheduler
    scheduler = ReduceLROnPlateau(
        optimizer,
        mode="min",
        factor=0.5,
        patience=5,
        verbose=True
    )
    
    # Create trainer
    trainer = FormFillingTrainer(
        model=model,
        tokenizer=tokenizer,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device
    )
    
    # Train model
    logger.info("Starting training...")
    trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=epochs,
        model_dir=model_dir,
        save_every=save_every,
        early_stopping=early_stopping
    )
    
    # Save final model
    save_model(model, model_dir / "final_model.pt")
    logger.info(f"Final model saved to {model_dir / 'final_model.pt'}")
    
    return model, tokenizer


if __name__ == "__main__":
    # Train model with default parameters
    train_model() 